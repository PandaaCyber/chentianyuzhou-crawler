import requests
import time
import os
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import html2text
from ebooklib import epub
import datetime
import json

class ChentianYuZhouCrawler:
    def __init__(self):
        self.base_url = "https://chentianyuzhou.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.articles = []
        self.h2t = html2text.HTML2Text()
        self.h2t.ignore_links = False
        self.h2t.ignore_images = False
        self.h2t.body_width = 0
        
    def get_page_content(self, url):
        """è·å–é¡µé¢å†…å®¹ï¼ŒåŒ…å«æ›´å¥½çš„é”™è¯¯å¤„ç†"""
        try:
            print(f"æ­£åœ¨è®¿é—®: {url}")
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å†…å®¹é•¿åº¦: {len(response.content)}")
            
            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘
            if response.url != url:
                print(f"é¡µé¢è¢«é‡å®šå‘åˆ°: {response.url}")
            
            return response
            
        except requests.exceptions.RequestException as e:
            print(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def analyze_page_structure(self, soup):
        """åˆ†æé¡µé¢ç»“æ„ï¼Œæå–æœ‰ç”¨ä¿¡æ¯"""
        print("åˆ†æé¡µé¢ç»“æ„...")
        
        # æå–é¡µé¢æ ‡é¢˜
        title = soup.find('title')
        if title:
            print(f"é¡µé¢æ ‡é¢˜: {title.get_text()}")
        
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        links = soup.find_all('a', href=True)
        print(f"æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
        
        # æŸ¥æ‰¾å¯èƒ½çš„æ–‡ç« å†…å®¹åŒºåŸŸ
        content_selectors = [
            '.post', '.article', '.content', '.entry',
            '[class*="post"]', '[class*="article"]', '[class*="content"]',
            'main', 'section', '.container', '.wrapper'
        ]
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                print(f"æ‰¾åˆ° {len(elements)} ä¸ª {selector} å…ƒç´ ")
        
        # æŸ¥æ‰¾æ–‡æœ¬å†…å®¹
        text_content = soup.get_text()
        print(f"é¡µé¢æ–‡æœ¬é•¿åº¦: {len(text_content)}")
        
        return links, text_content
    
    def extract_main_content(self, soup):
        """æå–é¡µé¢ä¸»è¦å†…å®¹"""
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()
        
        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = None
        content_selectors = [
            'main', 'article', '.main-content', '.content', '.post-content',
            '.entry-content', '.article-content', '#content', '#main'
        ]
        
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                main_content = element
                print(f"æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ: {selector}")
                break
        
        if not main_content:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šçš„å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨body
            main_content = soup.find('body')
            if main_content:
                print("ä½¿ç”¨bodyä½œä¸ºä¸»è¦å†…å®¹")
        
        return main_content
        
    def get_article_links(self):
        """è·å–æ‰€æœ‰æ–‡ç« é“¾æ¥"""
        print("æ­£åœ¨è·å–æ–‡ç« åˆ—è¡¨...")
        
        response = self.get_page_content(self.base_url)
        if not response:
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # åˆ†æé¡µé¢ç»“æ„
        links, text_content = self.analyze_page_structure(soup)
        
        # æå–é¡µé¢ä¸»è¦å†…å®¹ä½œä¸ºä¸€ç¯‡æ–‡ç« 
        main_content = self.extract_main_content(soup)
        if main_content:
            homepage_article = {
                'title': 'é™ˆå¤©å®‡å®™ - ä¸»é¡µå†…å®¹',
                'url': self.base_url,
                'content': self.h2t.handle(str(main_content)),
                'date': datetime.datetime.now().strftime('%Y-%m-%d')
            }
            self.articles.append(homepage_article)
        
        # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
        article_links = set()
        
        for link in links:
            href = link.get('href', '')
            if href:
                # æ„å»ºå®Œæ•´URL
                full_url = urljoin(self.base_url, href)
                
                # è¿‡æ»¤æ¡ä»¶
                if (full_url.startswith(self.base_url) and 
                    full_url != self.base_url and 
                    not any(x in full_url.lower() for x in [
                        '#', 'javascript:', 'mailto:', '.css', '.js', 
                        '.jpg', '.png', '.gif', '.pdf', '.zip'
                    ])):
                    
                    # æ£€æŸ¥é“¾æ¥æ–‡æœ¬ï¼Œçœ‹æ˜¯å¦åƒæ–‡ç« æ ‡é¢˜
                    link_text = link.get_text().strip()
                    if link_text and len(link_text) > 5:
                        article_links.add((full_url, link_text))
        
        print(f"æ‰¾åˆ° {len(article_links)} ä¸ªå¯èƒ½çš„æ–‡ç« é“¾æ¥")
        
        # é™åˆ¶çˆ¬å–æ•°é‡
        return list(article_links)[:10]
    
    def crawl_article(self, url, title_hint=None):
        """çˆ¬å–å•ç¯‡æ–‡ç« """
        try:
            print(f"æ­£åœ¨çˆ¬å–: {url}")
            response = self.get_page_content(url)
            if not response:
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è·å–æ ‡é¢˜
            title = title_hint
            if not title:
                title_selectors = ['h1', '.post-title', '.entry-title', 'title']
                for selector in title_selectors:
                    title_elem = soup.select_one(selector)
                    if title_elem:
                        title = title_elem.get_text().strip()
                        break
            
            if not title:
                title = f"æ–‡ç«  - {url.split('/')[-1]}"
            
            # è·å–å†…å®¹
            content = self.extract_main_content(soup)
            
            if content:
                # è½¬æ¢ä¸ºmarkdown
                markdown_content = self.h2t.handle(str(content))
                
                # æ¸…ç†markdownå†…å®¹
                markdown_content = self.clean_markdown(markdown_content)
                
                # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ„ä¹‰
                if len(markdown_content.strip()) < 100:
                    print(f"å†…å®¹å¤ªçŸ­ï¼Œå¯èƒ½ä¸æ˜¯æœ‰æ•ˆæ–‡ç« : {url}")
                    return None
                
                article_data = {
                    'title': title,
                    'url': url,
                    'content': markdown_content,
                    'date': datetime.datetime.now().strftime('%Y-%m-%d')
                }
                
                print(f"æˆåŠŸçˆ¬å–æ–‡ç« : {title}")
                return article_data
            
        except Exception as e:
            print(f"çˆ¬å–æ–‡ç« å¤±è´¥ {url}: {e}")
            return None
    
    def clean_markdown(self, content):
        """æ¸…ç†markdownå†…å®¹"""
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
        lines = [line.strip() for line in content.split('\n')]
        content = '\n'.join(lines)
        
        # ç§»é™¤è¿‡å¤šçš„é‡å¤å­—ç¬¦
        content = re.sub(r'(\*|-|=){10,}', r'\1\1\1', content)
        
        return content
    
    def save_markdown_files(self):
        """ä¿å­˜markdownæ–‡ä»¶"""
        if not os.path.exists('articles'):
            os.makedirs('articles')
        
        for i, article in enumerate(self.articles, 1):
            # æ¸…ç†æ–‡ä»¶å
            safe_title = re.sub(r'[^\w\s-]', '', article['title'])
            safe_title = re.sub(r'[-\s]+', '-', safe_title)
            filename = f"{i:03d}-{safe_title[:50]}.md"
            
            filepath = os.path.join('articles', filename)
            
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(f"# {article['title']}\n\n")
                    f.write(f"åŸæ–‡é“¾æ¥: {article['url']}\n")
                    f.write(f"çˆ¬å–æ—¥æœŸ: {article['date']}\n\n")
                    f.write("---\n\n")
                    f.write(article['content'])
                
                print(f"å·²ä¿å­˜: {filename}")
            except Exception as e:
                print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {filename}: {e}")
    
    def create_epub(self):
        """åˆ›å»ºEPUBç”µå­ä¹¦"""
        print("æ­£åœ¨åˆ›å»ºEPUBç”µå­ä¹¦...")
        
        # åˆ›å»ºepubå¯¹è±¡
        book = epub.EpubBook()
        
        # è®¾ç½®ä¹¦ç±ä¿¡æ¯
        book.set_identifier('chentianyuzhou-collection')
        book.set_title('é™ˆå¤©å®‡å®™ - æ”¯ä»˜å­¦ä¹ ç¤¾åŒºæ–‡ç« é›†åˆ')
        book.set_language('zh-CN')
        book.add_author('é™ˆå¤©å®‡å®™')
        book.add_metadata('DC', 'description', 'é™ˆå¤©å®‡å®™ç½‘ç«™æ–‡ç« é›†åˆï¼ŒåŒ…å«æ”¯ä»˜äº§å“ç»ç†ã€æŠ€æœ¯ã€æµ‹è¯•ã€å•†åŠ¡ç›¸å…³å†…å®¹')
        
        # æ·»åŠ å°é¢é¡µ
        intro_chapter = epub.EpubHtml(
            title='å‰è¨€',
            file_name='intro.xhtml',
            lang='zh-CN'
        )
        intro_content = f"""
        <html xmlns="http://www.w3.org/1999/xhtml">
        <head><title>å‰è¨€</title></head>
        <body>
        <h1>é™ˆå¤©å®‡å®™ - æ”¯ä»˜å­¦ä¹ ç¤¾åŒºæ–‡ç« é›†åˆ</h1>
        <p>æœ¬ç”µå­ä¹¦æ”¶å½•äº†é™ˆå¤©å®‡å®™ç½‘ç«™çš„ç›¸å…³å†…å®¹ã€‚</p>
        <p><strong>åŸç½‘ç«™åœ°å€:</strong> <a href="https://chentianyuzhou.com">https://chentianyuzhou.com</a></p>
        <p><strong>ç½‘ç«™ç®€ä»‹:</strong> æ”¯ä»˜å­¦ä¹ ç¤¾åŒºï¼Œæ”¯ä»˜äº§å“ç»ç†ã€æŠ€æœ¯ã€æµ‹è¯•ã€å•†åŠ¡éƒ½åœ¨çœ‹çš„æ”¯ä»˜å†…å®¹ç¤¾åŒº</p>
        <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        <p><strong>æ”¶å½•å†…å®¹:</strong> {len(self.articles)} ç¯‡</p>
        <hr/>
        <p><em>æ³¨ï¼šæœ¬ç”µå­ä¹¦ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œç‰ˆæƒå½’åŸä½œè€…æ‰€æœ‰ã€‚</em></p>
        </body>
        </html>
        """
        intro_chapter.content = intro_content
        book.add_item(intro_chapter)
        
        # åˆ›å»ºç›®å½•
        toc_items = [intro_chapter]
        spine = ['nav', intro_chapter]
        
        # æ·»åŠ æ–‡ç« ç« èŠ‚
        for i, article in enumerate(self.articles, 1):
            # åˆ›å»ºç« èŠ‚
            chapter = epub.EpubHtml(
                title=article['title'],
                file_name=f'chapter_{i:03d}.xhtml',
                lang='zh-CN'
            )
            
            # å°†markdownè½¬æ¢ä¸ºHTML
            chapter_content = f"""
            <html xmlns="http://www.w3.org/1999/xhtml">
            <head><title>{article['title']}</title></head>
            <body>
            <h1>{article['title']}</h1>
            <p><strong>åŸæ–‡é“¾æ¥:</strong> <a href="{article['url']}">{article['url']}</a></p>
            <p><strong>çˆ¬å–æ—¥æœŸ:</strong> {article['date']}</p>
            <hr/>
            """
            
            # ç®€å•çš„markdownåˆ°htmlè½¬æ¢
            md_content = article['content']
            md_content = md_content.replace('&', '&amp;')
            md_content = md_content.replace('<', '&lt;')
            md_content = md_content.replace('>', '&gt;')
            md_content = md_content.replace('\n\n', '</p><p>')
            md_content = f"<p>{md_content}</p>"
            md_content = re.sub(r'<p># (.*?)</p>', r'<h1>\1</h1>', md_content)
            md_content = re.sub(r'<p>## (.*?)</p>', r'<h2>\1</h2>', md_content)
            md_content = re.sub(r'<p>### (.*?)</p>', r'<h3>\1</h3>', md_content)
            md_content = re.sub(r'<p>\*\*(.*?)\*\*</p>', r'<p><strong>\1</strong></p>', md_content)
            md_content = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', md_content)
            
            chapter_content += md_content
            chapter_content += "\n</body>\n</html>"
            chapter.content = chapter_content
            
            book.add_item(chapter)
            toc_items.append(chapter)
            spine.append(chapter)
        
        # è®¾ç½®ç›®å½•
        book.toc = toc_items
        book.spine = spine
        
        # æ·»åŠ å¯¼èˆªæ–‡ä»¶
        book.add_item(epub.EpubNcx())
        book.add_item(epub.EpubNav())
        
        # å†™å…¥epubæ–‡ä»¶
        epub_filename = f'é™ˆå¤©å®‡å®™-æ”¯ä»˜å­¦ä¹ ç¤¾åŒº-{datetime.datetime.now().strftime("%Y%m%d")}.epub'
        
        try:
            epub.write_epub(epub_filename, book)
            print(f"EPUBç”µå­ä¹¦å·²åˆ›å»º: {epub_filename}")
            return epub_filename
        except Exception as e:
            print(f"åˆ›å»ºEPUBå¤±è´¥: {e}")
            return None
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        print("å¼€å§‹çˆ¬å–é™ˆå¤©å®‡å®™ç½‘ç«™...")
        print("ç½‘ç«™æè¿°: æ”¯ä»˜å­¦ä¹ ç¤¾åŒºï¼Œæ”¯ä»˜äº§å“ç»ç†ã€æŠ€æœ¯ã€æµ‹è¯•ã€å•†åŠ¡éƒ½åœ¨çœ‹çš„æ”¯ä»˜å†…å®¹ç¤¾åŒº")
        
        # è·å–æ–‡ç« é“¾æ¥
        article_links = self.get_article_links()
        
        # çˆ¬å–é¢å¤–çš„æ–‡ç« 
        for url, title_hint in article_links:
            if len(self.articles) >= 10:  # é™åˆ¶æ€»æ•°
                break
            article = self.crawl_article(url, title_hint)
            if article:
                self.articles.append(article)
            time.sleep(2)  # å¢åŠ å»¶è¿Ÿï¼Œæ›´ç¤¼è²Œ
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰è¶³å¤Ÿçš„å†…å®¹ï¼Œæ·»åŠ ä¸€ä¸ªè¯´æ˜æ–‡ç« 
        if len(self.articles) < 2:
            info_article = {
                'title': 'å…³äºé™ˆå¤©å®‡å®™ç½‘ç«™',
                'url': self.base_url,
                'content': f"""# å…³äºé™ˆå¤©å®‡å®™ç½‘ç«™

## ç½‘ç«™ä»‹ç»
é™ˆå¤©å®‡å®™æ˜¯ä¸€ä¸ªä¸“æ³¨äºæ”¯ä»˜é¢†åŸŸçš„å­¦ä¹ ç¤¾åŒºï¼Œä¸»è¦æœåŠ¡äºæ”¯ä»˜äº§å“ç»ç†ã€æŠ€æœ¯å¼€å‘ã€æµ‹è¯•å·¥ç¨‹å¸ˆã€å•†åŠ¡äººå‘˜ç­‰ç›¸å…³ä»ä¸šè€…ã€‚

## ç½‘ç«™åœ°å€
{self.base_url}

## ä¸»è¦å†…å®¹
- æ”¯ä»˜äº§å“ç»ç†ç›¸å…³å†…å®¹
- æ”¯ä»˜æŠ€æœ¯å¼€å‘çŸ¥è¯†
- æ”¯ä»˜æµ‹è¯•ç›¸å…³èµ„æ–™
- æ”¯ä»˜å•†åŠ¡çŸ¥è¯†åˆ†äº«

## çˆ¬å–è¯´æ˜
æœ¬æ¬¡çˆ¬å–äº {datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')} è¿›è¡Œã€‚

ç”±äºç½‘ç«™å¯èƒ½ä½¿ç”¨äº†åŠ¨æ€åŠ è½½æŠ€æœ¯æˆ–å…¶ä»–åçˆ¬æªæ–½ï¼Œéƒ¨åˆ†å†…å®¹å¯èƒ½æ— æ³•å®Œå…¨è·å–ã€‚å»ºè®®ç›´æ¥è®¿é—®åŸç½‘ç«™è·å–å®Œæ•´å†…å®¹ã€‚

## æŠ€æœ¯è¯´æ˜
ç½‘ç«™å¯èƒ½ä½¿ç”¨äº†ä»¥ä¸‹æŠ€æœ¯ï¼š
- JavaScriptåŠ¨æ€åŠ è½½
- å•é¡µé¢åº”ç”¨(SPA)
- å†…å®¹ä¿æŠ¤æœºåˆ¶
- ç™»å½•éªŒè¯

å¦‚éœ€è·å–å®Œæ•´å†…å®¹ï¼Œå»ºè®®ï¼š
1. ç›´æ¥è®¿é—®åŸç½‘ç«™
2. ä½¿ç”¨æµè§ˆå™¨çš„å¼€å‘è€…å·¥å…·æŸ¥çœ‹ç½‘ç»œè¯·æ±‚
3. è€ƒè™‘ä½¿ç”¨æ›´é«˜çº§çš„çˆ¬è™«å·¥å…·ï¼ˆå¦‚Seleniumï¼‰
""",
                'date': datetime.datetime.now().strftime('%Y-%m-%d')
            }
            self.articles.append(info_article)
        
        print(f"æˆåŠŸæ”¶é›† {len(self.articles)} ç¯‡å†…å®¹")
        
        # ä¿å­˜markdownæ–‡ä»¶
        self.save_markdown_files()
        
        # åˆ›å»ºepub
        epub_file = self.create_epub()
        
        print(f"ä»»åŠ¡å®Œæˆï¼")
        print(f"- ç”Ÿæˆäº† {len(self.articles)} ç¯‡æ–‡ç« çš„markdownæ–‡ä»¶")
        if epub_file:
            print(f"- åˆ›å»ºäº†EPUBç”µå­ä¹¦: {epub_file}")
        
        # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        import glob
        for file in glob.glob("articles/*.md"):
            print(f"  ğŸ“„ {file}")
        for file in glob.glob("*.epub"):
            print(f"  ğŸ“š {file}")

if __name__ == "__main__":
    crawler = ChentianYuZhouCrawler()
    crawler.run()
    
    def crawl_article(self, url):
        """çˆ¬å–å•ç¯‡æ–‡ç« """
        try:
            print(f"æ­£åœ¨çˆ¬å–: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è·å–æ ‡é¢˜
            title = None
            title_selectors = ['h1', '.post-title', '.entry-title', 'title']
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text().strip()
                    break
            
            if not title:
                title = f"æ–‡ç«  - {url.split('/')[-1]}"
            
            # è·å–å†…å®¹
            content = None
            content_selectors = [
                '.post-content',
                '.entry-content', 
                '.content',
                'article',
                '.post',
                'main'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem
                    break
            
            if not content:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹åŒºåŸŸï¼Œå°è¯•è·å–bodyä¸­çš„ä¸»è¦å†…å®¹
                content = soup.find('body')
            
            if content:
                # æ¸…ç†ä¸éœ€è¦çš„å…ƒç´ 
                for elem in content.select('script, style, nav, footer, header, .comment'):
                    elem.decompose()
                
                # è½¬æ¢ä¸ºmarkdown
                markdown_content = self.h2t.handle(str(content))
                
                # æ¸…ç†markdownå†…å®¹
                markdown_content = self.clean_markdown(markdown_content)
                
                article_data = {
                    'title': title,
                    'url': url,
                    'content': markdown_content,
                    'date': datetime.datetime.now().strftime('%Y-%m-%d')
                }
                
                return article_data
            
        except Exception as e:
            print(f"çˆ¬å–æ–‡ç« å¤±è´¥ {url}: {e}")
            return None
    
    def clean_markdown(self, content):
        """æ¸…ç†markdownå†…å®¹"""
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
        lines = [line.strip() for line in content.split('\n')]
        return '\n'.join(lines)
    
    def save_markdown_files(self):
        """ä¿å­˜markdownæ–‡ä»¶"""
        if not os.path.exists('articles'):
            os.makedirs('articles')
        
        for i, article in enumerate(self.articles, 1):
            # æ¸…ç†æ–‡ä»¶å
            safe_title = re.sub(r'[^\w\s-]', '', article['title'])
            safe_title = re.sub(r'[-\s]+', '-', safe_title)
            filename = f"{i:03d}-{safe_title[:50]}.md"
            
            filepath = os.path.join('articles', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"# {article['title']}\n\n")
                f.write(f"åŸæ–‡é“¾æ¥: {article['url']}\n")
                f.write(f"çˆ¬å–æ—¥æœŸ: {article['date']}\n\n")
                f.write("---\n\n")
                f.write(article['content'])
            
            print(f"å·²ä¿å­˜: {filename}")
    
    def create_epub(self):
        """åˆ›å»ºEPUBç”µå­ä¹¦"""
        print("æ­£åœ¨åˆ›å»ºEPUBç”µå­ä¹¦...")
        
        # åˆ›å»ºepubå¯¹è±¡
        book = epub.EpubBook()
        
        # è®¾ç½®ä¹¦ç±ä¿¡æ¯
        book.set_identifier('chentianyuzhou-collection')
        book.set_title('é™ˆå¤©å®‡å®™ - æ–‡ç« é›†åˆ')
        book.set_language('zh-CN')
        book.add_author('é™ˆå¤©å®‡å®™')
        
        # æ·»åŠ å°é¢é¡µ
        intro_chapter = epub.EpubHtml(
            title='å‰è¨€',
            file_name='intro.xhtml',
            lang='zh-CN'
        )
        intro_content = f"""
        <h1>é™ˆå¤©å®‡å®™ - æ–‡ç« é›†åˆ</h1>
        <p>æœ¬ç”µå­ä¹¦åŒ…å«äº†é™ˆå¤©å®‡å®™ç½‘ç«™çš„æ–‡ç« é›†åˆã€‚</p>
        <p>åŸç½‘ç«™åœ°å€: <a href="https://chentianyuzhou.com">https://chentianyuzhou.com</a></p>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}</p>
        <p>å…±æ”¶å½•æ–‡ç« : {len(self.articles)} ç¯‡</p>
        """
        intro_chapter.content = intro_content
        book.add_item(intro_chapter)
        
        # åˆ›å»ºç›®å½•
        toc_items = [intro_chapter]
        spine = ['nav', intro_chapter]
        
        # æ·»åŠ æ–‡ç« ç« èŠ‚
        for i, article in enumerate(self.articles, 1):
            # åˆ›å»ºç« èŠ‚
            chapter = epub.EpubHtml(
                title=article['title'],
                file_name=f'chapter_{i:03d}.xhtml',
                lang='zh-CN'
            )
            
            # å°†markdownè½¬æ¢ä¸ºHTML
            chapter_content = f"""
            <h1>{article['title']}</h1>
            <p><strong>åŸæ–‡é“¾æ¥:</strong> <a href="{article['url']}">{article['url']}</a></p>
            <p><strong>çˆ¬å–æ—¥æœŸ:</strong> {article['date']}</p>
            <hr/>
            """
            
            # ç®€å•çš„markdownåˆ°htmlè½¬æ¢
            md_content = article['content']
            # è¿™é‡Œå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„markdownè§£æå™¨ï¼Œä½†ä¸ºäº†ç®€åŒ–å°±åšåŸºæœ¬è½¬æ¢
            md_content = md_content.replace('\n\n', '</p><p>')
            md_content = f"<p>{md_content}</p>"
            md_content = re.sub(r'# (.*?)</p>', r'<h1>\1</h1>', md_content)
            md_content = re.sub(r'## (.*?)</p>', r'<h2>\1</h2>', md_content)
            md_content = re.sub(r'### (.*?)</p>', r'<h3>\1</h3>', md_content)
            
            chapter_content += md_content
            chapter.content = chapter_content
            
            book.add_item(chapter)
            toc_items.append(chapter)
            spine.append(chapter)
        
        # è®¾ç½®ç›®å½•
        book.toc = toc_items
        book.spine = spine
        
        # æ·»åŠ å¯¼èˆªæ–‡ä»¶
        book.add_item(epub.EpubNcx())
        book.add_item(epub.EpubNav())
        
        # å†™å…¥epubæ–‡ä»¶
        epub_filename = f'é™ˆå¤©å®‡å®™-æ–‡ç« é›†åˆ-{datetime.datetime.now().strftime("%Y%m%d")}.epub'
        epub.write_epub(epub_filename, book)
        
        print(f"EPUBç”µå­ä¹¦å·²åˆ›å»º: {epub_filename}")
        return epub_filename
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        print("å¼€å§‹çˆ¬å–é™ˆå¤©å®‡å®™ç½‘ç«™...")
        
        # è·å–æ–‡ç« é“¾æ¥
        article_links = self.get_article_links()
        
        # å¦‚æœåœ¨get_article_linksä¸­å·²ç»åˆ›å»ºäº†æµ‹è¯•æ–‡ç« ï¼Œç›´æ¥è·³åˆ°ä¿å­˜æ­¥éª¤
        if self.articles:
            print("ä½¿ç”¨é¢„åˆ›å»ºçš„æµ‹è¯•å†…å®¹")
        else:
            if not article_links:
                print("æ²¡æœ‰æ‰¾åˆ°æ–‡ç« é“¾æ¥ï¼Œåˆ›å»ºé»˜è®¤å†…å®¹")
                # åˆ›å»ºä¸€ä¸ªé»˜è®¤æ–‡ç« ç¡®ä¿æœ‰å†…å®¹è¾“å‡º
                default_article = {
                    'title': 'é™ˆå¤©å®‡å®™ç½‘ç«™è®¿é—®è®°å½•',
                    'url': self.base_url,
                    'content': f'# é™ˆå¤©å®‡å®™ç½‘ç«™\n\nç½‘ç«™åœ°å€: {self.base_url}\n\nè®¿é—®æ—¶é—´: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n\nè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„è®°å½•ï¼Œè¡¨ç¤ºçˆ¬è™«å·²ç»å°è¯•è®¿é—®äº†è¯¥ç½‘ç«™ã€‚',
                    'date': datetime.datetime.now().strftime('%Y-%m-%d')
                }
                self.articles.append(default_article)
            else:
                # çˆ¬å–æ–‡ç« 
                for url in article_links[:20]:  # é™åˆ¶çˆ¬å–æ•°é‡ï¼Œé¿å…è¿‡åº¦è¯·æ±‚
                    article = self.crawl_article(url)
                    if article:
                        self.articles.append(article)
                    time.sleep(1)  # ç¤¼è²Œæ€§å»¶è¿Ÿ
        
        if not self.articles:
            print("æ²¡æœ‰æˆåŠŸçˆ¬å–åˆ°ä»»ä½•æ–‡ç« ï¼Œåˆ›å»ºç©ºç™½æ–‡ç« ")
            empty_article = {
                'title': 'ç©ºç™½è®°å½•',
                'url': self.base_url,
                'content': 'æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« å†…å®¹',
                'date': datetime.datetime.now().strftime('%Y-%m-%d')
            }
            self.articles.append(empty_article)
        
        print(f"æˆåŠŸçˆ¬å– {len(self.articles)} ç¯‡æ–‡ç« ")
        
        # ä¿å­˜markdownæ–‡ä»¶
        self.save_markdown_files()
        
        # åˆ›å»ºepub
        epub_file = self.create_epub()
        
        print(f"ä»»åŠ¡å®Œæˆï¼ç”Ÿæˆäº† {len(self.articles)} ç¯‡æ–‡ç« çš„markdownæ–‡ä»¶å’Œepubç”µå­ä¹¦")
        print(f"EPUBæ–‡ä»¶: {epub_file}")
        
        # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        import glob
        for file in glob.glob("articles/*.md"):
            print(f"  {file}")
        for file in glob.glob("*.epub"):
            print(f"  {file}")

if __name__ == "__main__":
    crawler = ChentianYuZhouCrawler()
    crawler.run()
